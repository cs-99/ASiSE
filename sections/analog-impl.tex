\section{Analog Implementations}
This section provides an overview of selected implementations of NNs in past and recent research.
\subsection{Overview}
\subsubsection{Early Research}
As stated in \autoref{introduction} already, research on analog implementations of neural networks has been conducted many years ago. Graf et al. (1989) \cite{Graf1989} as one of those early works found some interesting aspects. First, the sum of products in a neuron is easily implemented with Kirchhoff's law computing the sum, while the product can be performed by a simple resistor. Second, interconnected (fully-connected) layers were difficult to create because of the high number of required connections, which would have led to a lot of space on the chip used up just for those layers. This point is not as prevalent today, as circuitry has decreased extremely in physical size since then. Another problem was the precision of the analog circuits, which was not as high back then. Therefore Graf et al. recommended to train the NN digitally before its analog implementation and to use a learning algorithm which can tolerate low precision network connections. Flexibility in terms of reprogrammable interconnections could be achieved by using a storage cell to contain the weight which then is applied to the actual connection element (e.g. resistor), but would use up a lot of space on the chip too. They concluded that designing an analog chip for a NN is a tradeoff between functionality and network size. If for example training should take place on the chip too or high resolution interconnections are required, the network must be smaller than without those features. Another conclusion was that neural networks in analog circuits can compute results way faster than their digital counterparts, but were not as flexible nor precise. An aspect not mentioned by Graf et al. is the electrical power consumption, which presumably was not as important at that time, because networks were not close to being as big as they are today.\\
Another example of pioneer work in this area is Zurada et al. \cite{Zurada1992}, who picked up on the problems of that time three years later and tried to combine the advantages of digital and analog implementations by adding a digital-to-analog interface to the chip which allowed the parameters and interconnection of the network to be programmable. Moreover, they implemented on-chip unsupervised (Hebbian\footnote{\url{https://julien-vitay.net/lecturenotes-neurocomputing/4-neurocomputing/5-Hebbian.html}}) learning as they assessed on-chip learning as being essential for most appliations. Even though their paper was a huge step forwars in terms of design of analog NNs, they correctly conclude that future designs will become way more complex as the networks get more complicated. Also, desirable properties for future NN chips are listed, such as being affordable, being reprogrammable and not taking up a lot of space. In contrast to Graf et al. power consumption as an important criteria is listed too.

\subsubsection{Recent Research}
Today, many NNs are trained and run on accelerators such as graphics processing units (GPUs\footnote{\url{https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html}}). This makes high flexibility in terms of network design possible while still having acceptable performance. However, it leads to high power consumption and hardware cost and can not reach the performance of an analog NN. With NNs getting a lot bigger to handle more complex tasks, this problem could get worse in the future. Therefore research efforts are made in the direction of analog NNs again. For example Schemmel et al. \cite{Schemmel2008} proposed an architecture to integrate programmable analog NNs on wafers which have low power consumption and high fault tolerance. A problem that is not fully solved yet is on-chip learning, as e.g. Krestinskaya et al. \cite{Krestinskaya2018} implemented the original backpropagation algorithm in analog circuits, however concluded that their implementation needs to be further optimized in terms of area usage and power consumption in the future.\\
Other papers propose further research in the topic of accelerators for NNs, which use parts realized in software and hardware to combine the benefits of each approach. So did Ambrogio et al. \cite{Ambrogio2018} in 2018. They achieved similar results as GPU trained NNs in terms of accuracy, while heavily outperforming them when it comes to computational speed and energy efficiency. Since field programmable gate arrays (FPGAs\footnote{\url{https://www.xilinx.com/products/silicon-devices/fpga/what-is-an-fpga.html}}), bridge the gap between hardware and software implementations in general, they are an obvious choice when it comes to accelerating a NN. Therefore Lu et al. \cite{Lu2019} proposed an accelerator based on a FPGA which operates on sparse CNNs. These were chosen, because removing unnecessary interconnections in a CNN (\emph{pruning}) is highly popular, especially for deep CNNs, to reduce the computational effort. Their experiments show that their accelerator architecture was way faster than previous ones for dense CNNs. When comparing with high-end GPUs, similar speeds could be achieved, while being more than 10 times as energy efficient. Because there are many publications about the topic of NN accelerators, Deng et al. \cite{Deng2020}, Bouvier et al. \cite{Bouvier2019} and Capra et al. \cite{Capra2020} created surveys discussing this topic. Both state that the hardware needs to be optimized further as algorithms get more complex and the demand for neural networks increases. Especially engergy efficient architectures are important for further research, where memory, because it is very engergy consuming, needs to play a big role in.\\
Fully on-chip implemented NNs were discussed by Yingge et al. \cite{Yingge2020}. They conclude that further research needs to be done in terms of NN structure and processing technology to increase their efficiency and lower production cost. Also they claim, that dedicated development environments for (D)NNs would improve the designing process by making it simpler and yielding more results.