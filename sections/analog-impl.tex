\section{Analog Implementations}
This section provides an overview of selected analog implementations of NNs in past and recent research. After that advantages and disadvantages of this implementation method can be summarized.
\subsection{Overview}
\subsubsection{Early Research}
As stated in \autoref{introduction} already, research on analog implementations of neural networks has been conducted many years ago. Graf et al. (1989) \cite{Graf1989} as one of those early works found some interesting aspects. First, the sum of products in a neuron is easily implemented with Kirchhoff's law computing the sum, while the product can be performed by a simple resistor. Second, interconnected (fully-connected) layers were difficult to create because of the high number of required connections, which would have led to a lot of space on the chip used up just for those layers. This point is not as prevalent today, as circuitry has decreased extremely in physical size since then. Another problem was the precision of the analog circuits, which was not as high back then. Therefore Graf et al. recommended to train the NN digitally before its analog implementation and to use a learning algorithm which can tolerate low precision network connections. Flexibility in terms of reprogrammable interconnections could be achieved by using a storage cell to contain the weight which then is applied to the actual connection element (e.g. resistor), but would use up a lot of space on the chip too. They concluded that designing an analog chip for a NN is a tradeoff between functionality and network size. If for example training should take place on the chip too or high resolution interconnections are required, the network must be smaller than without those features. Another conclusion was that neural networks in analog circuits can compute results way faster than their digital counterparts, but were not as flexible nor precise. An aspect not mentioned by Graf et al. is the electrical power consumption, which presumably was not as important at that time, because networks were not close to being as big as they are today.\\
Another example of pioneer work in this area is Zurada et al. \cite{Zurada1992}, who picked up on the problems of that time three years later and tried to combine the advantages of digital and analog implementations by adding a digital-to-analog interface to the chip which allowed the parameters and interconnection of the network to be programmable. Moreover, they implemented on-chip unsupervised (Hebbian\footnote{\url{https://julien-vitay.net/lecturenotes-neurocomputing/4-neurocomputing/5-Hebbian.html}}) learning as they assessed on-chip learning as being essential for most appliations. Even though their paper was a huge step forwars in terms of design of analog NNs, they correctly conclude that future designs will become way more complex as the networks get more complicated. Also, desirable properties for future NN chips are listed, such as being affordable, being reprogrammable and not taking up a lot of space. In contrast to Graf et al. power consumption as an important criteria is listed too.

\subsubsection{Recent Research}