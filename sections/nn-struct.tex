\section{Neural Network Structure}
Since for implementing a NN whether its digital or analog the structure is crucial, this section provides a brief summary about basic NN components. Because NNs try to recreate the structure of a brain, there are similarities between the two, however, these are not relevant for implementation and therefore not explained in this paper.
\subsection{Neuron}
\label{neuron}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{neuron}
	\caption{Structure of a neuron. The inputs $x_i$ are multiplied with their corresponding $w_i$, after that, the bias $b$ is added. Lastly an activation function is applied to determine the output $x'$.}
	\label{fig:neuron}
\end{figure}
The smallest piece of a neural network is a \emph{neuron} (also called \emph{perceptron}), whose structure is shown in \autoref{fig:neuron}. It sums up an arbitrary number of input values $x_i$ multiplied with their individual weights $w_i$ and adds a bias $b$ to it. The resulting value is called the \emph{activation} $a$ and gets passed to the next neuron (or the output) after applying the \emph{activation function} $f$. This function plays a huge role in the networks performance, can be selected almost arbitrary and is a research topic on its own. However, simpler activation functions tend to outperform more complex ones, presumably because of a more difficult training process (see \autoref{training}). Currently the most widespread function is the \emph{Rectified Linear Unit} which is defined as $ReLU(a) = max(0,a)$. \cite{Ramachandran2017}


\subsection{Layer}
\begin{figure}[]
	\centering
	\includegraphics[scale=0.5]{network}
	\caption{Visualization of a neural network with only fully connected layers. Each input (blue) and  output (red) is connected to every neuron in the hidden layer (green).}
	\label{fig:network}
\end{figure}
The NN itself consists out of multiple \emph{layers} which in turn consist of the neurons described in \autoref{neuron}. \autoref{fig:network} shows a basic network with three layers, one of which is considered a \emph{hidden layer}, which means that it is neither input nor output. Some common layer types are explained in the following subsections.

\subsubsection{Fully-connected Layer}
A \emph{fully-connected layer} denotes a layer, where each neuron consumes input from each neuron of the preceding layer as depicted in \autoref{fig:network}. Having a lot of fully connected layers can lead to higher processing time and electrical power requirements due to a high number of trainable parameters, even though all the inputs might not be required in most of the layers \cite{Capra2020}.

\subsubsection{Convolutional Layer}
\begin{figure}[]
	\centering
	\includegraphics[scale=0.6]{conv-layer}
	\caption{Schematic drawing of a convolutional layer. The two-dimensional input is convolved with a 3$\times$3 kernel to calculate a 2$\times$2 feature map.}
	\label{fig:conv-layer}
\end{figure}
\label{layer:conv}
\emph{Convolutional layers} as shown in \autoref{fig:conv-layer} are used to exract features from their input. Therefore, they use a \emph{kernel}, which stands for an array of weights which is smaller than the input. This kernel can then be applied to multiple positions of the input (by "shifting" over it) with a convolution. This leads to a so-called \emph{feature map}. Since the kernel is smaller than the input, convolutional layers are good at detecting for example features of an image (e.g. eyes of a human) regardless of their position. If the output is not supposed to be smaller than the input,  \emph{padding} can be used to extend the input, for example by using extrapolation. Typically multiple convolutional layers will be used in parallel to extract multiple features. As this layer type is the core technique, it is name giving for \emph{convolutional neural networks} (CNNs). Layers which perform the inverse of convolutional ones are called \emph{de-convolutional layers}. \cite{Guo2017}

\subsubsection{Pooling Layer}
\begin{figure}[]
	\centering
	\includegraphics[scale=0.6]{pool-layer}
	\caption{Schematic drawing of a max pooling layer. It extracts the maximum value from a 2$\times$2 array of the two-dimensional input on 4 different positions.}
	\label{fig:pool-layer}
\end{figure}
\emph{Pooling layers} reduce the size of their input by performing a simple pooling operation, such as selecting the maximum (\emph{max pooling}) or average (\emph{average pooling}) value, on different regions of the input. An example of a max pooling layer can be seen inÂ¸ \autoref{fig:pool-layer}. These layers typically follow convolutional ones, as they can be used to extract the existance of a feature in a feature map independent of its position and therefore add robustness for example when rotating or translating an input image. \cite{Guo2017}

\subsubsection{Normalization Layer}
If activation functions such as ReLU (see \autoref{neuron}) are used in a neural network, inputs into the following layers can take on very different values. Thus, normalizing data inside the network in \emph{normalization layers} can have significant benefits. It allows an easier analysis by the model and speeds up training, as the layers after the normalization one can adapt to a single distribution of inputs. \cite{Capra2020}


\subsection{Classification}
As already explained in \autoref{layer:conv}, CNNs with their convolutional layers are highly performant when extracting features is required. This makes them especially applicable in topics such as computer vision, image classification, video processing or speech recognition \cite{Bhatt2021}. \emph{Deep neural networks} stand for NNs with many hidden layers, while \emph{recurrent neural networks} (sometimes also called \emph{feed-backward NNs}) correspond to NNs, which unlike the up to this point only discussed \emph{feed-forward NNs} contain feedback connections to previous layers or inbetween them. Having feedback inside the network allows better processing of event sequences or time-based data, thus fitting those NNs better to applications such as language learning or adaptive processes in autonomous systems \cite{Medsker1999}. As classification of NNs is not the main scope, it needs to be noticed that there are many other types of NNs, which are not further discussed in this paper.
