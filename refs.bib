@Article{Ambrogio2018,
  author    = {Ambrogio, Stefano and Narayanan, Pritish and Tsai, Hsinyu and Shelby, Robert M and Boybat, Irem and Di Nolfo, Carmelo and Sidler, Severin and Giordano, Massimo and Bodini, Martina and Farinha, Nathan CP and others},
  journal   = {Nature},
  title     = {Equivalent-accuracy accelerated neural-network training using analogue memory},
  year      = {2018},
  number    = {7708},
  pages     = {60--67},
  volume    = {558},
  file      = {:pdfs/s41586-018-0180-5.pdf:PDF},
  publisher = {Nature Publishing Group},
}

@Article{Graf1989,
  author  = {Graf, H.P. and Jackel, L.D.},
  journal = {IEEE Circuits and Devices Magazine},
  title   = {Analog electronic neural network circuits},
  year    = {1989},
  number  = {4},
  pages   = {44-49},
  volume  = {5},
  doi     = {10.1109/101.29902},
  file    = {:pdfs/Analog_electronic_neural_network_circuits.pdf:PDF},
}

@InProceedings{Schemmel2008,
  author    = {Schemmel, Johannes and Fieres, Johannes and Meier, Karlheinz},
  booktitle = {2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)},
  title     = {Wafer-scale integration of analog neural networks},
  year      = {2008},
  pages     = {431-438},
  doi       = {10.1109/IJCNN.2008.4633828},
  file      = {:pdfs/Wafer-scale_integration_of_analog_neural_networks.pdf:PDF},
}

@Article{Yao2020,
  author    = {Yao, Peng and Wu, Huaqiang and Gao, Bin and Tang, Jianshi and Zhang, Qingtian and Zhang, Wenqiang and Yang, J Joshua and Qian, He},
  journal   = {Nature},
  title     = {Fully hardware-implemented memristor convolutional neural network},
  year      = {2020},
  number    = {7792},
  pages     = {641--646},
  volume    = {577},
  file      = {:pdfs/s41586-020-1942-4.pdf:PDF},
  publisher = {Nature Publishing Group},
}

@Article{Deng2020,
  author  = {Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
  journal = {Proceedings of the IEEE},
  title   = {Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey},
  year    = {2020},
  number  = {4},
  pages   = {485-532},
  volume  = {108},
  doi     = {10.1109/JPROC.2020.2976475},
  file    = {:pdfs/Model_Compression_and_Hardware_Acceleration_for_Neural_Networks_A_Comprehensive_Survey.pdf:PDF},
}

@InProceedings{Vittoz1990,
  author    = {Vittoz, E.A.},
  booktitle = {IEEE International Symposium on Circuits and Systems},
  title     = {Analog VLSI implementation of neural networks},
  year      = {1990},
  pages     = {2524-2527 vol.4},
  doi       = {10.1109/ISCAS.1990.112524},
  file      = {:pdfs/Analog_VLSI_implementation_of_neural_networks.pdf:PDF},
}

@InProceedings{Li2018,
  author    = {Li, Youjie and Park, Jongse and Alian, Mohammad and Yuan, Yifan and Qu, Zheng and Pan, Peitian and Wang, Ren and Schwing, Alexander and Esmaeilzadeh, Hadi and Kim, Nam Sung},
  booktitle = {2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  title     = {A Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks},
  year      = {2018},
  pages     = {175-188},
  doi       = {10.1109/MICRO.2018.00023},
  file      = {:pdfs/A_Network-Centric_Hardware_Algorithm_Co-Design_to_Accelerate_Distributed_Training_of_Deep_Neural_Networks.pdf:PDF},
}

@Article{Capra2020,
  author  = {Capra, Maurizio and Bussolino, Beatrice and Marchisio, Alberto and Masera, Guido and Martina, Maurizio and Shafique, Muhammad},
  journal = {IEEE Access},
  title   = {Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead},
  year    = {2020},
  pages   = {225134-225180},
  volume  = {8},
  doi     = {10.1109/ACCESS.2020.3039858},
  file    = {:pdfs/Hardware_and_Software_Optimizations_for_Accelerating_Deep_Neural_Networks_Survey_of_Current_Trends_Challenges_and_the_Road_Ahead.pdf:PDF},
}

@Article{Bouvier2019,
  author     = {Bouvier, Maxence and Valentian, Alexandre and Mesquida, Thomas and Rummens, Francois and Reyboz, Marina and Vianello, Elisa and Beigne, Edith},
  journal    = {J. Emerg. Technol. Comput. Syst.},
  title      = {Spiking Neural Networks Hardware Implementations and Challenges: A Survey},
  year       = {2019},
  issn       = {1550-4832},
  month      = {apr},
  number     = {2},
  volume     = {15},
  abstract   = {Neuromorphic computing is henceforth a major research field for both academic and industrial actors. As opposed to Von Neumann machines, brain-inspired processors aim at bringing closer the memory and the computational elements to efficiently evaluate machine learning algorithms. Recently, spiking neural networks, a generation of cognitive algorithms employing computational primitives mimicking neuron and synapse operational principles, have become an important part of deep learning. They are expected to improve the computational performance and efficiency of neural networks, but they are best suited for hardware able to support their temporal dynamics. In this survey, we present the state of the art of hardware implementations of spiking neural networks and the current trends in algorithm elaboration from model selection to training mechanisms. The scope of existing solutions is extensive; we thus present the general framework and study on a case-by-case basis the relevant particularities. We describe the strategies employed to leverage the characteristics of these event-driven algorithms at the hardware level and discuss their related advantages and challenges.},
  address    = {New York, NY, USA},
  articleno  = {22},
  doi        = {10.1145/3304103},
  file       = {:pdfs/3304103.pdf:PDF},
  issue_date = {April 2019},
  keywords   = {hardware implementation, neuromorphic computing, spiking neural networks, event driven, spiking, neural network, Neuromorphic computing, hardware, machine learning},
  numpages   = {35},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3304103},
}

@InProceedings{Lu2019,
  author    = {Lu, Liqiang and Xie, Jiaming and Huang, Ruirui and Zhang, Jiansong and Lin, Wei and Liang, Yun},
  booktitle = {2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  title     = {An Efficient Hardware Accelerator for Sparse Convolutional Neural Networks on FPGAs},
  year      = {2019},
  pages     = {17-25},
  doi       = {10.1109/FCCM.2019.00013},
  file      = {:pdfs/An_Efficient_Hardware_Accelerator_for_Sparse_Convolutional_Neural_Networks_on_FPGAs.pdf:PDF},
}

@InProceedings{Yingge2020,
  author    = {Yingge, Huo and Ali, Imran and Lee, Kang-Yoon},
  booktitle = {2020 IEEE International Conference on Big Data and Smart Computing (BigComp)},
  title     = {Deep Neural Networks on Chip - A Survey},
  year      = {2020},
  pages     = {589-592},
  doi       = {10.1109/BigComp48618.2020.00016},
  file      = {:pdfs/Deep_Neural_Networks_on_Chip_-_A_Survey.pdf:PDF},
}

@Article{Zurada1992,
  author  = {Zurada, J.M.},
  journal = {IEEE Circuits and Devices Magazine},
  title   = {Analog implementation of neural networks},
  year    = {1992},
  number  = {5},
  pages   = {36-41},
  volume  = {8},
  doi     = {10.1109/101.158511},
  file    = {:pdfs/Analog_implementation_of_neural_networks.pdf:PDF},
}

@Article{Schwartz1989,
  author  = {Schwartz, D.B. and Howard, R.E. and Hubbard, W.E.},
  journal = {IEEE Journal of Solid-State Circuits},
  title   = {A programmable analog neural network chip},
  year    = {1989},
  number  = {2},
  pages   = {313-319},
  volume  = {24},
  doi     = {10.1109/4.18590},
  file    = {:pdfs/A_programmable_analog_neural_network_chip.pdf:PDF},
}

 Copy | Download

@Article{Harrer1992,
  author  = {Harrer, H. and Nossek, J.A. and Stelzl, R.},
  journal = {IEEE Transactions on Neural Networks},
  title   = {An analog implementation of discrete-time cellular neural networks},
  year    = {1992},
  number  = {3},
  pages   = {466-476},
  volume  = {3},
  doi     = {10.1109/72.129419},
  file    = {:pdfs/An_analog_implementation_of_discrete-time_cellular_neural_networks.pdf:PDF},
}

@InProceedings{Krestinskaya2018,
  author    = {Krestinskaya, Olga and Salama, Khaled Nabil and James, Alex Pappachen},
  booktitle = {2018 IEEE International Symposium on Circuits and Systems (ISCAS)},
  title     = {Analog Backpropagation Learning Circuits for Memristive Crossbar Neural Networks},
  year      = {2018},
  pages     = {1-5},
  doi       = {10.1109/ISCAS.2018.8351344},
  file      = {:pdfs/Analog_Backpropagation_Learning_Circuits_for_Memristive_Crossbar_Neural_Networks.pdf:PDF},
}

@Misc{Ramachandran2017,
  author     = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  title      = {Searching for Activation Functions},
  year       = {2017},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {10.48550/ARXIV.1710.05941},
  file       = {:pdfs/1710.05941.pdf:PDF},
  keywords   = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  publisher  = {arXiv},
  readstatus = {read},
  url        = {https://arxiv.org/abs/1710.05941},
}

@InProceedings{Guo2017,
  author     = {Guo, Tianmei and Dong, Jiwen and Li, Henjian and Gao, Yunxing},
  booktitle  = {2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
  title      = {Simple convolutional neural network on image classification},
  year       = {2017},
  pages      = {721-724},
  doi        = {10.1109/ICBDA.2017.8078730},
  file       = {:pdfs/Simple_convolutional_neural_network_on_image_classification.pdf:PDF},
  readstatus = {read},
}

@Article{Bhatt2021,
  author         = {Bhatt, Dulari and Patel, Chirag and Talsania, Hardik and Patel, Jigar and Vaghela, Rasmika and Pandya, Sharnil and Modi, Kirit and Ghayvat, Hemant},
  journal        = {Electronics},
  title          = {CNN Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope},
  year           = {2021},
  issn           = {2079-9292},
  number         = {20},
  volume         = {10},
  abstract       = {Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep CNN (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for CNN study. Several inspirational concepts for the progress of CNN have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep CNN. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep CNN architectures, and it divides numerous recent developments in CNN architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based CNN are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in CNN by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the CNNâ€™s components, the strengths and weaknesses of various CNN variants, research gap or open challenges, CNN applications, and the future research direction.},
  article-number = {2470},
  doi            = {10.3390/electronics10202470},
  file           = {:pdfs/electronics-10-02470.pdf:PDF},
  readstatus     = {read},
  url            = {https://www.mdpi.com/2079-9292/10/20/2470},
}

@Book{Medsker1999,
  author     = {Medsker, Larry and Jain, Lakhmi C},
  publisher  = {CRC press},
  title      = {Recurrent neural networks: design and applications},
  year       = {1999},
  readstatus = {read},
}

@Article{Wilamowski2010,
  author     = {Wilamowski, Bogdan M. and Yu, Hao},
  journal    = {IEEE Transactions on Neural Networks},
  title      = {Neural Network Learning Without Backpropagation},
  year       = {2010},
  number     = {11},
  pages      = {1793-1803},
  volume     = {21},
  doi        = {10.1109/TNN.2010.2073482},
  readstatus = {read},
}

@Article{Werbos1990,
  author     = {Werbos, P.J.},
  journal    = {Proceedings of the IEEE},
  title      = {Backpropagation through time: what it does and how to do it},
  year       = {1990},
  number     = {10},
  pages      = {1550-1560},
  volume     = {78},
  doi        = {10.1109/5.58337},
  readstatus = {read},
}

@Comment{jabref-meta: databaseType:bibtex;}
